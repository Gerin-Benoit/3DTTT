{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This tutorial steps through the model evaluation process for the case of the baseline ensemble MC dropout.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from tabulate import tabulate\n",
    "import torch\n",
    "\n",
    "print(\"Torch  Version\", torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.utils.auxiliary_functions import *\n",
    "from src.utils.assessment import calc_uncertainty_regection_curve, f_beta_metrics, get_model_errors, get_performance_metric\n",
    "from src.utils.uncertainty import ensemble_uncertainties_regression\n",
    "from src.utils.plot_utils import get_comparison_error_retention_plot, get_comparison_f1_retention_plot\n",
    "from src.models.mc_dropout import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional: Customize the plots configuration\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "plt.rc('figure', titlesize=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Directories to be used"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial assumes that\n",
    "* all data files are in a local directory named `datasets`.\n",
    "* the trained models and train statistics for data scaling are in a local folder named `baselines`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = \"datasets\"\n",
    "model_dir = \"baselines\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data partitions and assign them to dictionary data_all\n",
    "data_all = {}\n",
    "\n",
    "for partition in [\"train\", \"dev_in\", \"dev_out\"]:\n",
    "    df = load_data_and_set_index(filepath=os.path.join(data_dir, f\"{partition}.csv\"), index_column_name=\"time_id\")\n",
    "    data_all[partition] = df\n",
    "\n",
    "data_all[\"dev\"] = pd.concat([data_all[\"dev_in\"], data_all[\"dev_out\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Number of records per dataset type:\" + \"\\n\")\n",
    "for k in data_all.keys():\n",
    "    print(f\"{k}: \", data_all[k].shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_features = [\"draft_aft_telegram\",\n",
    "                  \"draft_fore_telegram\",\n",
    "\n",
    "                  \"stw\",\n",
    "                  \"diff_speed_overground\",\n",
    "\n",
    "                  \"awind_vcomp_provider\",\n",
    "                  \"awind_ucomp_provider\",\n",
    "                  \"rcurrent_vcomp\",\n",
    "                  \"rcurrent_ucomp\",\n",
    "                  \"comb_wind_swell_wave_height\",\n",
    "\n",
    "                  \"timeSinceDryDock\",\n",
    "                  ]\n",
    "\n",
    "target = \"power\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load statistics for data scaling\n",
    "filepath = os.path.join(model_dir, 'stats')\n",
    "\n",
    "with open(os.path.join(filepath, 'means.json'), 'r') as f:\n",
    "    means = pd.Series(json.load(f))\n",
    "with open(os.path.join(filepath, 'stds.json'), 'r') as f:\n",
    "    stds = pd.Series(json.load(f))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_all_norm = {}\n",
    "for k in data_all.keys():\n",
    "    data_all_norm[k] = data_normalization(data=data_all[k], means=means, stds=stds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_all_norm['train'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The provided baseline is an ensemble MC dropout consisting of 10 probabilistic MC dropout neural networks that have the same structure but trained with different parameters' initialization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loaded_models = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    model = ProbMCdropoutDNN(input_size=len(input_features),\n",
    "                             hidden_size_1=50,\n",
    "                             hidden_size_2=20,\n",
    "                             dropout=0.005)\n",
    "    load_path = os.path.join(model_dir, f\"member_{i}\", \"best_model.pth\")\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    loaded_models.append(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During inference, each member of the ensemble is sampled 10 times (multi_runs=10) to capture the epistemic uncertainty due to the stochasticity of the single modelâ€™s parameters)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "\n",
    "for k in data_all.keys():\n",
    "    predictions[k] = {}\n",
    "    inputs = torch.tensor(data_all_norm[k][input_features].values).float()\n",
    "    preds_norm = get_ensemble_predictions(model_list=loaded_models,\n",
    "                                          data_norm=inputs,\n",
    "                                          multi_runs=10)\n",
    "    predictions[k][\"norm\"] = preds_norm\n",
    "\n",
    "    # Denormalize predicted mean\n",
    "    preds_denorm = denorm_prediction(preds_norm=preds_norm,\n",
    "                                     target_mean=means[target],\n",
    "                                     target_std=stds[target])\n",
    "    predictions[k][\"denorm\"] = preds_denorm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uncertainties = {}\n",
    "for k in data_all.keys():\n",
    "    uncertainties[k] = ensemble_uncertainties_regression(preds=predictions[k][\"denorm\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sqr_errors = {}\n",
    "for k in data_all.keys():\n",
    "    ens_predictions = np.squeeze(np.mean(predictions[k][\"denorm\"][:, :, 0], axis=0))\n",
    "    power_labels = np.asarray(data_all[k][target])\n",
    "    sqr_errors[k] = get_model_errors(y_pred=ens_predictions, y_true=power_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MSE retention values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "plt.subplots_adjust(wspace=.3, hspace=.4)\n",
    "\n",
    "for i, partition in enumerate([\"train\", \"dev_in\", \"dev_out\"]):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    get_comparison_error_retention_plot(error=sqr_errors[partition],\n",
    "                                        uncertainty=uncertainties[partition][\"tvar\"])\n",
    "    plt.ylabel('MSE(KW)')\n",
    "    plt.xlabel(\"Retention Fraction\")\n",
    "    plt.legend(loc=\"upper left\", prop={\"size\": 12})\n",
    "    plt.title(partition, pad=25)\n",
    "    plt.ylim(-0.1e6, 2.5e6)\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F1 retention curves"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define threshold of acceptable errors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use an acceptable error threshold of (500 kW)^2\n",
    "thresh = 500 ** 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot F1 retentions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "plt.subplots_adjust(wspace=.3, hspace=.4)\n",
    "\n",
    "for i, partition in enumerate([\"train\", \"dev_in\", \"dev_out\"]):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    get_comparison_f1_retention_plot(error=sqr_errors[partition],\n",
    "                                     uncertainty=uncertainties[partition][\"tvar\"],\n",
    "                                     threshold=thresh)\n",
    "    plt.legend(loc=\"upper left\", prop={\"size\": 12})\n",
    "    plt.ylabel('F1')\n",
    "    plt.xlabel(\"Retention Fraction\")\n",
    "    plt.title(partition, pad=25)\n",
    "    plt.ylim(-0.1, 1.7)\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictive performance Tables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "table = [[\"Data\", \"RMSE Ens. (KW)\", \"MAE Ens. (KW)\", \"MAPE (%)\"]]\n",
    "\n",
    "for k in data_all.keys():\n",
    "    ens_predictions = np.squeeze(np.mean(predictions[k][\"denorm\"][:, :, 0], axis=0))\n",
    "    rmse_metric = get_performance_metric(y_pred=ens_predictions, y_true=data_all[k][target], metric=\"rmse\")\n",
    "    mae_metric = get_performance_metric(y_pred=ens_predictions, y_true=data_all[k][target], metric=\"mae\")\n",
    "    mape_metric = get_performance_metric(y_pred=ens_predictions, y_true=data_all[k][target], metric=\"mape\")\n",
    "\n",
    "    results = [k,\n",
    "               np.round(rmse_metric, 0),\n",
    "               np.round(mae_metric, 0),\n",
    "               np.round(100 * mape_metric, 2)]\n",
    "    table.append(results)\n",
    "\n",
    "print(\"Classic metrics\")\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "table = [[\"Data\", \"R-AUC\", \"F1-AUC\", \"F1 @ 95 %\"]]\n",
    "\n",
    "for k in data_all.keys():\n",
    "    r = sqr_errors[k]\n",
    "    var = uncertainties[k][\"tvar\"]\n",
    "\n",
    "    rejection_mse = calc_uncertainty_regection_curve(errors=r, uncertainty=var, group_by_uncertainty=False)\n",
    "    retention_mse = rejection_mse[::-1]\n",
    "    retention_fractions = np.linspace(0, 1, len(retention_mse))\n",
    "    roc_auc = auc(x=retention_fractions[::-1], y=retention_mse)\n",
    "\n",
    "    f_auc, f95, retention_f1 = f_beta_metrics(errors=r,\n",
    "                                              uncertainty=var,\n",
    "                                              threshold=thresh,\n",
    "                                              beta=1.0)\n",
    "    results = [k,\n",
    "               roc_auc,\n",
    "               np.round(f_auc, 3),\n",
    "               np.round(f95, 3)]\n",
    "    table.append(results)\n",
    "\n",
    "print(\"AUCs\")\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shifts_39",
   "language": "python",
   "name": "shifts_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}